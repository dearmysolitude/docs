## 용어
>[!quote] IBM docs
머신 러닝, 딥 러닝, 신경망은 모두 인공 지능의 하위 분야입니다. 그러나 신경망은 실제로 머신 러닝의 하위 분야이며, 딥 러닝은 신경망의 하위 분야입니다.
- 머신러닝: 레이블링 된 정형 데이터를 활용하여 예측을 수행. 비정형 데이터도 사용하지만 전처리를 거친다.
- 딥러닝: 머신 러닝에 일반적으로 필요로하는 전처리 과정을 일부 사용하지 않아도 된다. 텍스트나 이미지 같은 비정형 데이터를 수집/처리할 수 있으며, 특성 추출을 자동화한다.
### 딥러닝 알고리즘
대표적인 회귀 분석 모형임. 여러 계층을 사용하므로 속도가 느리지만 정확도는 높다.
ANN
DNN
### Classification(분류) & Forecast(예측) 문제
- k-NN
- SVM: 가상의 mulit dimensional plane으로 경계를 나눈다.
- Decsision Tree
### 데이터가 부족할 때?
- 데이터가 부족할 때 속성 값들의 제곱을  속성을 사용하는 기본적인 기술.
- Polynomial Features
## 실습
### 뉴스 기사 카테고리 분류 모델
개요: 제목에서 주요한 단어를 추출하여 관련도 높은 카테고리로 분류하는 분류 모델
데이터셋: https://www.kaggle.com/datasets/khoshbayani/news-texts
1. 데이터 전처리
  - 결측치 제거
  - 문장/단어 토큰화
    - 불용어 제거
    - 소문자 통일
    - 특수분자 제거
    - 명사/동사/형용사/부사 구분: 동사, 명사 추출(Lemmatization)
2. 자연어 처리
  - 제목에서 추출한 명사/동사를 TF-IDF 벡터화
3. 벡터화된 토큰들과 카테고리 매핑을 통한 머신러닝: KNN, SVM, RandomForest 모델의 비교
  - 하이퍼파라미터 최적화: RandomForest모델에 대하여.
### 뉴스 카테고리 분류 모델 개선
인공지능 모델 개선을 위한 방법: scikit learn을 사용하여.
> [!note] 모델 정확도 높이기
> - 데이터 전처리(Data pre-processing): 불용어 제거, 대소문자 통일, 토큰화, 스테밍/렘마타이제이션
> - 특징 엔지니어링(Feature Engineering): 제목 외의 다른 특징, 메타데이터를 추가
> - 모델 선택 및 하이퍼파라미터 튜닝(Hyperparameter Tunning): 로지스틱 회귀, 서포트 벡터 머신, 램덤 포레스트 등. 교차 검증(cross-validation)을 통해 하이퍼파라미터 최적화
> - 앙상블 기법(Ensanble): 여러 모델을 결합(Bagging, boosting, stacking,...)
> - 더 많은 데이터 확보: 데이터셋을 추가하거나 data augmentation 기법 사용
> - 전이학습(Transfer Learning): 대규모 corpus로 사전 학습된 언어 모델(BERT, ELMo 등)을 사용하여 특징을 추출하고 분류 모델을 fine-tuning하는 전이학습 기법을 적용
#### 초기 개선안
- 더 큰 데이터셋 사용: https://www.kaggle.com/datasets/rmisra/news-category-dataset
- 특징 엔지니어링: feature을 추가하여 모델의 성능을 늘림: 기존에 있던 제목에 기사 extract를 추가하여 별도의 독립 변수로 설정.
- 결과: Colab GPU 사용량 초과! → 20만 개 RandomTree, 분류 속성 20 여개
#### 2차 개선안
처음부터 많은 데이터를 쑤셔넣지 말고 다른 접근법을 생각해보자.
- 분류 속성을 10개 이내로, 제목만 데이터셋으로하여 돌려보자. 일단 데이터 수 자체가 증가했으니.
## Hyper parameter
정확도를 높이기 위한 모델 조정. _현재 최댓값이 그래프 전체 범위에서 최댓값이 아닐수도 있다._
- Gradient
- Learning Rate
### 과적합
Overfitting. 어떤 입력을 넣어도 
## F1 Score

## AutoML

> [!note] AutoDL
> Hyperparameter가 많다보니 외려 직접 돌려보는 것이 효율적.
## 참고 자료
[BERT 논문 정리](https://tmaxai.github.io/post/BERT/)