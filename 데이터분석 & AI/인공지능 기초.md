## 용어
>[!quote] IBM docs
머신 러닝, 딥 러닝, 신경망은 모두 인공 지능의 하위 분야입니다. 그러나 신경망은 실제로 머신 러닝의 하위 분야이며, 딥 러닝은 신경망의 하위 분야입니다.
- 머신러닝: 레이블링 된 정형 데이터를 활용하여 예측을 수행. 비정형 데이터도 사용하지만 전처리를 거친다.
- 딥러닝: 머신 러닝에 일반적으로 필요로하는 전처리 과정을 일부 사용하지 않아도 된다. 텍스트나 이미지 같은 비정형 데이터를 수집/처리할 수 있으며, 특성 추출을 자동화한다.
### 딥러닝 알고리즘
대표적인 회귀 분석 모형임. 여러 계층을 사용하므로 속도가 느리지만 정확도는 높다.
ANN
DNN
### Classification(분류) & Forecast(예측) 문제
- k-NN
- SVM: 가상의 mulit dimensional plane으로 경계를 나눈다.
- Decsision Tree
### 데이터가 부족할 때?
- 데이터가 부족할 때 속성 값들의 제곱을  속성을 사용하는 기본적인 기술.
- Polynomial Features
## 실습
### 뉴스 기사 카테고리 분류 모델
개요: 제목에서 주요한 단어를 추출하여 관련도 높은 카테고리로 분류하는 분류 모델
데이터셋: https://www.kaggle.com/datasets/khoshbayani/news-texts
1. 데이터 전처리
  - 결측치 제거
  - 문장/단어 토큰화
    - 불용어 제거
    - 소문자 통일
    - 특수분자 제거
    - 명사/동사/형용사/부사 구분: 동사, 명사 추출(Lemmatization)
2. 자연어 처리
  - 제목에서 추출한 명사/동사를 TF-IDF 벡터화
3. 벡터화된 토큰들과 카테고리 매핑을 통한 머신러닝: KNN, SVM, RandomForest 모델의 비교
  - 하이퍼파라미터 최적화: RandomForest모델에 대하여.
### 뉴스 카테고리 분류 모델 개선
인공지능 모델 개선을 위한 방법: scikit learn을 사용하여.
> [!note] 모델 정확도 높이기
> - 데이터 전처리(Data pre-processing): 불용어 제거, 대소문자 통일, 토큰화, 스테밍/렘마타이제이션
> - 특징 엔지니어링(Feature Engineering): 제목 외의 다른 특징, 메타데이터를 추가
> - 모델 선택 및 하이퍼파라미터 튜닝(Hyperparameter Tunning): 로지스틱 회귀, 서포트 벡터 머신, 램덤 포레스트 등. 교차 검증(cross-validation)을 통해 하이퍼파라미터 최적화
> - 앙상블 기법(Ensanble): 여러 모델을 결합(Bagging, boosting, stacking,...)
> - 더 많은 데이터 확보: 데이터셋을 추가하거나 data augmentation 기법 사용
> - 전이학습(Transfer Learning): 대규모 corpus로 사전 학습된 언어 모델(BERT, ELMo 등)을 사용하여 특징을 추출하고 분류 모델을 fine-tuning하는 전이학습 기법을 적용
#### 초기 개선안
- 더 큰 데이터셋 사용: https://www.kaggle.com/datasets/rmisra/news-category-dataset
- 특징 엔지니어링: feature을 추가하여 모델의 성능을 늘림: 기존에 있던 제목에 기사 extract를 추가하여 별도의 독립 변수로 설정.
- 결과: Colab GPU 사용량 초과! → 20만 개 RandomTree, 분류 속성 20 여개
#### 2차 개선안
처음부터 많은 데이터를 쑤셔넣지 말고 다른 접근법을 생각해보자.
- 분류 속성을 10개 이내로, 제목만 데이터셋으로하여 돌려보자. 일단 데이터 수 자체가 증가했으니.
## Hyper parameter
정확도를 높이기 위한 모델 조정. _현재 최댓값이 그래프 전체 범위에서 최댓값이 아닐수도 있다._
- Gradient
- Learning Rate
### 과적합
Overfitting. 어떤 입력을 넣어도 
## F1 Score

## AutoML

> [!note] AutoDL
> Hyperparameter가 많다보니 외려 직접 돌려보는 것이 효율적.
## 참고 자료
[BERT 논문 정리](https://tmaxai.github.io/post/BERT/)

## 용어
- 회귀 Regression 연속적인 값을 예측하는 문제 (예: 집값 예측)
- 판단 Binary Classification 두 개의 클래스 중 하나를 예측하는 문제 (예: 스팸 메일 여부)
- 분류 Classification 여러 클래스 중 하나를 예측하는 문제 (예: 꽃 종류 분류)
    
- 머신러닝 Machine learning 데이터로부터 패턴을 학습하여 의사결정을 하는 AI 기술
- 딥러닝 Deep Learning 신경망을 깊게 쌓아 복잡한 패턴을 학습하는 머신러닝의 한 분야
- 생성형ai creative AI 새로운 콘텐츠를 생성할 수 있는 AI 기술 (예: 이미지 생성, 텍스트 생성)
    
- 모델 model 데이터의 패턴을 학습하고 예측을 수행하는 알고리즘
- 가중치 weight 모델 내에서 각 입력의 중요도를 나타내는 값
- 학습전 모델은 훈련을 통해 최적의 가중치를 찾는다. 모델의 구조와 형식을 알고 있다면, 가중치만 알면 학습한 모델을 쉽게 복원할 수 있다.

- 피쳐 Feature 모델의 입력으로 사용되는 데이터의 특성
- 타겟 Target 모델이 예측하고자 하는 목표 변수
    
- 훈련셋 Train set 모델을 학습시키는 데 사용되는 데이터
- 테스트셋 Test set 학습된 모델의 성능을 평가하는 데 사용되는 데이터
    
- 선형 회귀분석 LinearRegression 입력 변수와 출력 변수 사이의 선형 관계를 모델링하는 통계적 방법
    
- loss function 모델의 예측과 실제 값의 차이를 측정하는 함수. 다양한 산출 방법이 있지만 가장 쉬운 두 가지 방법이 mse와 mae이다. 
- error 예측값과 실제값의 차이
- MSE(Mean Squared Error) 오차의 제곱 평균
- MAE(Mean Absolute Error) 오차의 절대값 평균
    
- Optimize 모델의 성능을 향상시키기 위해 파라미터를 조정하는 과정
    
- 경사 하강법 Gradient Descent 손실 함수를 최소화하기 위해 파라미터를 조정하는 최적화 알고리즘. 한가지 가중치에 대하여 손실 함수의 값을 최소화하는 가중치 값을 산출하는 방식으로 모델의 가중치를 찾는다.
	- 원래 경사하강법은 함수 값이 낮아지는 방향으로 독립 변수 값을 변형시켜가면서 최종적으로는 최소 함수 값을 갖도록 하는 독립 변수 값을 찾는 방법이다: 즉 최소값을 찾는 방법이며, 인공지능 분야에서는 손실 함수의 최소값을 찾는 방법으로 가중치의 최적값을 산출하는 데에 사용한다.
	- 각 독립 변수에 대한 편미분을 진행 했을 때, 기울기값과 크기를  고려하여 최소값을 찾는 방향으로 독립변수를 이동하여 종속변수()를 산출한다.
	- 참고: [링크](https://angeloyeo.github.io/2020/08/16/gradient_descent.html)
- optimize=“sgd” 확률적 경사 하강법, 일부 데이터만 사용하여 최적화
- optimize=“adam” 적응형 학습률을 사용하는 최적화 알고리즘